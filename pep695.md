# Implementing PEP 695

[PEP 695](https://peps.python.org/pep-0695/), Type Parameter Syntax, is a major change
to Python's syntax and scoping rules. This document is a discussion of how I am
implementing this change at runtime in
[PR #103764](https://github.com/python/cpython/pull/103764). It is not a discussion of
whether the PEP is a good idea, or a deep dive into the type system concepts that the
PEP supports. I will discuss how the PEP's new syntax behaves at runtime and why. To
motivate that behavior, I will also briefly discuss the typing constructs for which the
PEP provides syntax.

I'll first recapitulate the new syntax added by the PEP, then go over each of the major
areas that the implementation touches:

- The parser: How do we make the new syntax work in a backward-compatible way and with
  good error messages?
- Scoping: How do we make the new constructs behave in an intuitive way that is
  consistent with the rest of the language?
- Lazy evaluation: How do we avoid the pitfalls with eager evaluation of type
  annotations for the new syntax?
- The interpreter: How does the new syntax work at runtime?
- Runtime objects: How are `TypeVar`, `Generic`, etc. represented now that they are
  created from C code rather than Python code?

## User-visible semantics

The PEP's motivation is to improve the syntax for generic functions, classes, and type
aliases in Python.

### Generic functions

A generic function, broadly, is one where there is a relation between the types of the
arguments and the return type, or between multiple of the arguments.

As an example, let's consider the Python builtin `max` function:

```pycon
>>> max(["a", "b"])
'b'
>>> max([1, 2])
2
>>> max([1.0, 2.0])
2.0
```

It takes an iterable of values of the same type, let's call it `T`, and returns one of
them, also of type `T`.

Under PEP 695, we could write this signature as:

```python
def max[T](args: Iterable[T]) -> T:
    ...
```

(The real signature is more complicated, but we'll leave that to
[typeshed](https://github.com/python/typeshed/blob/9457de310a089cd0340521b11c31808a1d27e40d/stdlib/builtins.pyi#L1459).)

Here, `T` is a _type parameter_ that parameterizes the type of the `max` function. When
a type checker sees a usage of this function, it essentially replaces the type parameter
with a more concrete type, such as `int` or `str`.

### Generic classes

In addition to functions, classes can be generic. Generic classes are often containers
of some sort: they contain elements, and to describe them in the type system we want to
say what kinds of elements they contain. For example, `[1, 2, 3]` and `["a", "b", "c"]`
are both lists, but one contains ints and the other strings. We write the two types as
`list[int]` and `list[str]`.

If we were to define the `list` class in Python, we could write it (simplified) like
this using the PEP's syntax:

```python
class list[T]:
    def __getitem__(self, index: int, /) -> T:
        ...

    def append(self, element: T) -> None:
        ...
```

### Type aliases

Last, type aliases can be generic. A type alias is an alternative name for a complex
type. For example, if you often have functions that take either lists or sets containing
some particular type, you might write:

```python
type ListOrSet[T] = list[T] | set[T]
```

The PEP also introduces syntax for non-generic aliases. For example, many functions
accept either a `str` or a path-like object representing a path. We can represent this
type as:

```python
type StrPath = str | os.PathLike[str]
```

### `TypeVarTuple` and `ParamSpec`

So far we have only seen the simplest kind of type parameter: an unconstrained type
variable (or `TypeVar`). The Python type system supports a few more kinds of type
parameters. In addition to plain `TypeVar`, there are `TypeVarTuple` and `ParamSpec`,
and `TypeVar` (but not the other two) can have _bounds_ and _constraints_.

`TypeVarTuple` is created by writing a single asterisk (`*`) before the name of a type
parameter (e.g., `def func[*Ts](): ...`). This object was introduced in
[PEP 646](https://peps.python.org/pep-0646/) and is intended primarily for representing
the types of multidimensional arrays, such as numpy arrays. `ParamSpec` is created with
two asterisks (`**`), e.g. `def func[**P](): ...`. It was introduced by
[PEP 612](https://peps.python.org/pep-0612/) and is most useful for typing complex
decorators. Both of these have very subtle behavior in the type system, but their
runtime behavior is quite simple, so we won't cover them in much detail here.

### Bounds and constraints

Bounds and constraints exist because many generics don't work with all types, but only
with some specific set of types. For example, one of the ways the above signature for
`max` is incomplete is that `max` only works on types that support comparison:

```
>>> max([2j, 3j])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: '>' not supported between instances of 'complex' and 'complex'
```

If we have a type `SupportsRichComparison` that represents types that support operators
like `>`, we can type `max` as:

```
def max[T: SupportsRichComparison](args: Iterable[T]) -> T:
    ...
```

Now type checkers will reject calls like `max([2j, 3j])`.

In addition to bounds, type variables support constraints, which express that a type
variable can only represent some specific set of types:

```
def add[T: (int, str, bytes)](a: T, b: T) -> T:
    return a + b
```

## Parser and AST

### Grammar

The PEP gives the word `type` a specific syntactic meaning: it introduces a type alias.
Before Python 3.9, this would have been very disruptive: we would have had to make
`type` into a full-fledged keyword, and break all of the existing code that uses the
`type()` builtin. However, Python's new PEG-based parser has great support for _soft
keywords_: keywords that apply only in specific contexts. Python already had a few soft
keywords to support pattern matching, and this PEP adds `type`.

Python's parser is written in a nice, declarative way, so the new syntax only required a
few dozen lines of new code: a new case in the `simple_stmt` rule to represent type
aliases, a new optional type parameters list on the grammar rules for classes,
functions, and async functions, and new grammar production to parse aliases and type
parameter lists.

Still, there are a few subtleties. First, let's talk about the new statement rule:

```
simple_stmt[stmt_ty] (memo):
    | assignment
    | &"type" type_alias
    | e=star_expressions { _PyAST_Expr(e, EXTRA) }
    | &'return' return_stmt
```

(Many more options omitted.) The `&"type"` here is a performance optimization: it tells
the parser to only bother looking at this option if the next token is `"type"`. Notice
that we have `"type"` in double quotes but `'return'` in single quotes: that is because
`type` is a soft keyword and `return` is a hard keyword.

The order of the rules also matters. We might want to put type aliases further down the
list because they are expected to be relatively rare compared to say `return`
statements, but the grammar doesn't work out if we put the type alias rule below the
`star_expressions` rule, because that rule will throw an error on type alias syntax
before the parser gets a chance to try out the `type_alias` rule. That is why
`type_alias` is in the second position in the list.

Improving error messages has been a major emphasis in recent Python releases. I tried to
produce informative errors for some plausible mistakes:

```pycon
>>> def f[*Ts: int](): pass
  File "<stdin>", line 1
    def f[*Ts: int](): pass
             ^^^^^
SyntaxError: cannot use bound with TypeVarTuple
```

In the future, we may add more specific error rules for other cases as we learn more
about common mistakes users make with this syntax.

### AST

The output of Python's parser is an abstract syntax tree (AST). Implementing the PEP
required the creation of a few new AST nodes to represent type aliases and type
parameters. There was one area of disagreement here: should the name of a type alias be
represented as a plain identifier in the AST, or as a full-fledged `ast.Name` node? The
former is simpler and represents the grammar more closely, in that the name is only
allowed to be a single identifier, but the latter allows the AST to retain precise
location information about the name, which makes the job of some static analysis tools
easier. Therefore, we chose to go with `ast.Name`, and the AST representation of type
aliases is `TypeAlias(expr name, typeparam* typeparams, expr value)`.

### Distinguishing bounds and constraints

The definition of a `TypeVar` in the AST is as follows:
`TypeVar(identifier name, expr? bound)`. But as we discussed above, `TypeVar`\ s can
also have constraints, represented syntactically as a tuple of types.

How do we distinguish between them? One option could be to push the distinction all the
way to the runtime: we evaluate the value, and if it is a tuple, we treat it as a set of
constraints, and otherwise we treat it as a bound. This option would lead to a subtle
incompatibility between static and runtime type checking:

```python
types = (bytes, str)

def func[T: types](): ...
```

A static type checker would reject this code, because `types` looks like a bound but is
not a valid type. However, tool that looks at `func` at runtime would have no good way
to distinguish this invalid function from a valid declaration such as
`def func[T: (bytes, str)](): ...`.

Therefore, we want to distinguish between bounds and constraints at compile time,
without evaluating the expression. One way to do that would be to push the distinction
to the grammar and have separate grammar rules for bounds and constraints. I didn't try
to implement that, but I expect it would be relatively complex, as we'd have to
re-implement tuple parsing just for this case.

Instead, we distinguish between the two cases later, in the compiler: if the expression
in the `bound` field is syntactically a tuple (an instance of `ast.Tuple`), we treat it
as a set of constraints, otherwise we treat it as a bound.

## Lazy evaluation

When annotations were introduced in Python 3.0, they were evaluated eagerly in the scope
in which they are defined. This is easy to understand and implement, but it caused
problems when annotations became widely used for typing. Users with large codebases saw
that a big chunk of their import time was spent evaluating type annotations that they'd
never look at again, because they were used only for static typing. In addition, type
annotations often refer to names that haven't yet been defined when the annotation is
evaluated. For example, users expect to be able to use the name of a class as a type
annotation within the class itself, but when the class body executes, the name of the
class is not yet defined. The workaround for this problem was to write the class name in
a string: `"Class"`.

To fix these problems, [PEP 563](https://peps.python.org/pep-0563/) introduced a
mechanism that turns all annotations into strings in the compiler, so that annotations
become much cheaper and users don't have to worry about whether names they use in
annotations are defined at runtime. This was scheduled to become the only behavior in
Python 3.10, but it was belatedly discovered that the change would break some runtime
uses of type annotations, which had become unexpectedly popular. An alternative
solution, [PEP 649](https://peps.python.org/pep-0649/) was introduced. It essentially
wraps all annotations in a function that is evaluated only on demand. This gets us the
best of both worlds: we don't pay the cost to evaluate all the annotations at import
time, we can use forward references without issues, and we can still introspect type
annotations at runtime. This behavior is slated to be implemented in Python 3.13.

PEP 695 introduces two syntactic contexts that are similar to annotations in that they
are expected to contain types: the value of type aliases and the bounds or constraints
of type variables. Initially, the PEP proposed to evaluate these eagerly, with a
special-cased mechanism to support evaluation of recursive type aliases. However, when I
started implementing the PEP, I felt this risked repeating the same mistakes that we
made with type annotations, and those mistakes are expected to take many more years to
fully fix. By this time, it was clear that some variation of PEP 649 was likely to be
the long-term future for type annotations, so it made sense to use PEP 649-like
semantics for these new syntactic constructs from the beginning.

For both type aliases and bounds, there are common use cases where forward references
are required. Most obviously, type aliases may be recursive:

```python
type Expr = int | Add[Expr, Expr] | Subtract[Expr, Expr]
```

The original version of the PEP handled this with a special case, where the name of the
type alias was already defined during the evaluation of the value. However, this trick
would not work if there are multiple mutually recursive type aliases:

```python
from typing import Literal

type BinOp = Literal["+", "-"]
type LeftParen = Literal["("]
type RightParen = Literal[")"]
type SimpleExpr = int | Parenthesized
type Parenthesized = tuple[LeftParen, Expr, RightParen]
type Expr = SimpleExpr | tuple[SimpleExpr, BinOp, Expr]
```

This set of types represents a simple grammar supporting integers literals, binary
operators (`a + b` and `a - b`), and parenthesized expressions (`a - (b - c)`).

Forward references have historically been very common in `TypeVar` bounds to represent
methods that return instances of the current class. The `Self` special form from
[PEP 673](https://peps.python.org/pep-0673/) has made many of these bounds obsolete, but
forward references in bounds remain occasionally useful. For example, consider
interconnected `Connection` and `Cursor` classes implementing a database engine:

```python
class Connection:
    # Allow the user to provide a subclass of Cursor to control cursor creation
    def cursor[CursorT: Cursor](self, cursor_class: type[CursorT]) -> CursorT:
        ...

class Cursor:
    @property
    def connection(self) -> Connection:
        ...
```

Both classes have type annotations that refer to each other, and lazy evaluation of the
bound and the type annotations leaves the programmer free to order the two classes in
whatever way they prefer.

The initial implementation of lazy evaluation is relatively simple: when a type alias
object is created, we create a function that encapsulates the evaluation of the value of
the alias. This function is stored on the type alias object, and called when the user
requests the value of the type alias (e.g., by accessing the `.__value__` attribute).
The same idea applies to `TypeVar` bounds and constraints.

PEP 649 introduces more advanced mechanisms for evaluating annotations that deal with
cases where some of the names may not be defined and with use cases that prefer a
stringified version of the annotations. When PEP 649 is implemented, we will add similar
evaluation mechanisms to PEP 695's lazily evaluated values, so that users can treat them
in the same way as they treat annotations.

## Scoping

### Requirements

The new scoping rules are the most subtle aspect of the runtime implementation of the
PEP. Indeed, the PEP says that "The lexical scope introduced by the new type parameter
syntax is unlike traditional scopes". The scoping semantics are motivated by several
important use cases:

- Multiple objects in the same module must be able to use the same type parameter names
  without interfering with each other. If we define both `def max[T](...): ...` and
  `def min[T](...): ...` in one module, the two type parameters `T` should be
  independent of each other and scoped only to the two function declarations. This helps
  users understand the semantics that type checkers use for type parameters.
- Type parameters must be usable within the scope of the generic. A function that is
  generic over `T` must be able to use `T` not only in its type annotations, but also in
  annotations for its local variables or for nested functions. A class that is generic
  over `T` may use `T` in its list of bases, in the annotations for class variables and
  methods, and for local variables and nested functions within its methods.
- As discussed above, the values of type aliases and the bounds/constraints of type
  variables should be lazily evaluated, but evaluating them later should behave as
  expected.
- Type parameters must be usable at runtime: code within the lexical scope of a type
  parameter must be able to use the parameter's name and get a usable object back. This
  is important for tools that access types at runtime.

I wrote a new section of the PEP,
[Scoping Behavior](https://peps.python.org/pep-0695/#scoping-behavior), that describes
the intended scoping semantics in detail. It was informed heavily by the implementation
strategy I ended up adopting, which is what we'll discuss next.

### Implementation strategies

While implementing the PEP, I went through several possible implementation strategies
before I landed on one that worked well:

- The PEP itself suggests treating type parameters as an "overlay" over the scope in
  which they are defined, which temporarily adds some new names that are only visible
  within the "overlay" scope. I had trouble getting this to work because to be visible
  in inner scopes, the names needed to be cell variables, and the overlay scope didn't
  provide an obvious place to put the cell variables.
- So perhaps instead we should put the type parameters in the enclosing scope, but
  mangle their names so that they are unique. I tried this, but because of the design of
  Python's symbol table implementation, it proved difficult to get the mangling right in
  all nested scopes. There are also edge cases where I'm not even sure this could be
  implemented correctly, such as if a name is conditionally defined in a class
  namespace.

### Lambda lifting

I eventually landed on a technique where we use a special, immediately evaluated
function to define the type parameters. I later learned this technique is called "lambda
lifting". Essentially, we desugar:

```python
def func[T](arg: T): ...
```

Into:

```python
def __generic_parameters_of_func():
    T = TypeVar("T")
    def func(arg: T): ...
    return func
func = __generic_parameters_of_func()
```

For the most part, this gives us the semantics we need without the introduction of
complex new concepts into the symtable and compiler: the semantics for function scopes
and nonlocals already work in the way that we want scopes for type parameters to work.

However, there was one big wrinkle and a few smaller ones. Let's start with the big one:
class scopes.

### Class scopes

### Other issues

- Walrus, yield, await
- Qualnames
- Nonlocal

### Lazy evaluation

Same issues with class scopes, same solution.

## Interpreter

- New opcodes for class scopes
- Many new intrinsics

## Runtime objects

- TypeVar etc. now in C
- Aiming for full compatibility
- Delegate to Python for some operations (most of all, for Generic)
