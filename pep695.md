# Implementing PEP 695

[PEP 695](https://peps.python.org/pep-0695/), Type Parameter Syntax, is a major change
to Python's syntax and scoping rules. This document is a discussion of how I am
implementing this change at runtime in
[PR #103764](https://github.com/python/cpython/pull/103764). It is not a discussion of
whether the PEP is a good idea, or a deep dive into the type system concepts that the
PEP supports. Instead, I will discuss how the PEP's new syntax behaves at runtime, why
it behaves that way, and how we are implementing it. To motivate that behavior, I will
also briefly discuss the typing constructs for which the PEP provides syntax.

I'll first recapitulate the new syntax added by the PEP, then go over each of the major
areas that the implementation touches:

- The parser: How do we make the new syntax work in a backward-compatible way and with
  good error messages?
- Scoping: How do we make the new constructs behave in an intuitive way that is
  consistent with the rest of the language?
- Lazy evaluation: How do we avoid the pitfalls with eager evaluation of type
  annotations for the new syntax?
- The interpreter: How does the new syntax work at runtime?
- Runtime objects: How are `TypeVar`, `Generic`, etc. represented now that they are
  created from C code rather than Python code?

## User-visible semantics

The PEP's motivation is to improve the syntax for generic functions, classes, and type
aliases in Python.

### Generic functions

A generic function, broadly, is one where there is a relation between the types of the
arguments and the return type, or between multiple of the arguments.

As an example, let's consider the Python builtin `max` function:

```pycon
>>> max(["a", "b"])
'b'
>>> max([1, 2])
2
>>> max([1.0, 2.0])
2.0
```

It takes an iterable of values of the same type, let's call it `T`, and returns one of
them, also of type `T`.

Under PEP 695, we could write this signature as:

```python
def max[T](args: Iterable[T]) -> T:
    ...
```

(The real signature is more complicated, but we'll leave that to
[typeshed](https://github.com/python/typeshed/blob/9457de310a089cd0340521b11c31808a1d27e40d/stdlib/builtins.pyi#L1459).)

Here, `T` is a _type parameter_ that parameterizes the type of the `max` function. When
a type checker sees a usage of this function, it essentially replaces the type parameter
with a more concrete type, such as `int` or `str`.

### Generic classes

In addition to functions, classes can be generic. Generic classes are often containers
of some sort: they contain elements, and to describe them in the type system we want to
say what kinds of elements they contain. For example, `[1, 2, 3]` and `["a", "b", "c"]`
are both lists, but one contains ints and the other strings. We write the two types as
`list[int]` and `list[str]`.

If we were to define the `list` class in Python, we could write it (simplified) like
this using the PEP's syntax:

```python
class list[T]:
    def __getitem__(self, index: int, /) -> T:
        ...

    def append(self, element: T) -> None:
        ...
```

### Type aliases

Last, type aliases can be generic. A type alias is an alternative name for a complex
type. For example, if you often have functions that take either lists or sets containing
some particular type, you might write:

```python
type ListOrSet[T] = list[T] | set[T]
```

The PEP also introduces syntax for non-generic aliases. For example, many functions
accept either a `str` or a path-like object representing a path. We can represent this
type as:

```python
type StrPath = str | os.PathLike[str]
```

### `TypeVarTuple` and `ParamSpec`

So far we have only seen the simplest kind of type parameter: an unconstrained type
variable (or `TypeVar`). The Python type system supports a few more kinds of type
parameters. In addition to plain `TypeVar`, there are `TypeVarTuple` and `ParamSpec`,
and `TypeVar` (but not the other two) can have _bounds_ and _constraints_.

`TypeVarTuple` is created by writing a single asterisk (`*`) before the name of a type
parameter (e.g., `def func[*Ts](): ...`). This object was introduced in
[PEP 646](https://peps.python.org/pep-0646/) and is intended primarily for representing
the types of multidimensional arrays, such as numpy arrays. `ParamSpec` is created with
two asterisks (`**`), e.g. `def func[**P](): ...`. It was introduced by
[PEP 612](https://peps.python.org/pep-0612/) and is most useful for typing complex
decorators. Both of these have very subtle behavior in the type system, but their
runtime behavior is quite simple, so we won't cover them in much detail here.

### Bounds and constraints

Bounds and constraints exist because many generics don't work with all types, but only
with some specific set of types. For example, one of the ways the above signature for
`max` is incomplete is that `max` only works on types that support comparison:

```
>>> max([2j, 3j])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: '>' not supported between instances of 'complex' and 'complex'
```

If we have a type `SupportsRichComparison` that represents types that support operators
like `>`, we can type `max` as:

```
def max[T: SupportsRichComparison](args: Iterable[T]) -> T:
    ...
```

Now type checkers will reject calls like `max([2j, 3j])`.

In addition to bounds, type variables support constraints, which express that a type
variable can only represent some specific set of types:

```
def add[T: (int, str, bytes)](a: T, b: T) -> T:
    return a + b
```

## Parser and AST

### Grammar

The PEP gives the word `type` a specific syntactic meaning: it introduces a type alias.
Before Python 3.9, this would have been very disruptive: we would have had to make
`type` into a full-fledged keyword, and break all of the existing code that uses the
`type()` builtin. However, Python's new PEG-based parser has great support for _soft
keywords_: keywords that apply only in specific contexts. Python already had a few soft
keywords to support pattern matching, and this PEP adds `type`.

Python's parser is written in a nice, declarative way, so the new syntax only required a
few dozen lines of new code: a new case in the `simple_stmt` rule to represent type
aliases, a new optional type parameters list on the grammar rules for classes,
functions, and async functions, and new grammar production to parse aliases and type
parameter lists.

Still, there are a few subtleties. First, let's talk about the new statement rule:

```
simple_stmt[stmt_ty] (memo):
    | assignment
    | &"type" type_alias
    | e=star_expressions { _PyAST_Expr(e, EXTRA) }
    | &'return' return_stmt
```

(Many more options omitted.) The `&"type"` here is a performance optimization: it tells
the parser to only bother looking at this option if the next token is `"type"`. Notice
that we have `"type"` in double quotes but `'return'` in single quotes: that is because
`type` is a soft keyword and `return` is a hard keyword.

The order of the rules also matters. We might want to put type aliases further down the
list because they are expected to be relatively rare compared to say `return`
statements, but the grammar doesn't work out if we put the type alias rule below the
`star_expressions` rule, because that rule will throw an error on type alias syntax
before the parser gets a chance to try out the `type_alias` rule. That is why
`type_alias` is in the second position in the list.

Improving error messages has been a major emphasis in recent Python releases. I tried to
produce informative errors for some plausible mistakes:

```pycon
>>> def f[*Ts: int](): pass
  File "<stdin>", line 1
    def f[*Ts: int](): pass
             ^^^^^
SyntaxError: cannot use bound with TypeVarTuple
```

In the future, we may add more specific error rules for other cases as we learn more
about common mistakes users make with this syntax.

### AST

The output of Python's parser is an abstract syntax tree (AST). Implementing the PEP
required the creation of a few new AST nodes to represent type aliases and type
parameters. There was one area of disagreement here: should the name of a type alias be
represented as a plain identifier in the AST, or as a full-fledged `ast.Name` node? The
former is simpler and represents the grammar more closely, in that the name is only
allowed to be a single identifier, but the latter allows the AST to retain precise
location information about the name, which makes the job of some static analysis tools
easier. Therefore, we chose to go with `ast.Name`, and the AST representation of type
aliases is `TypeAlias(expr name, typeparam* typeparams, expr value)`.

### Distinguishing bounds and constraints

The definition of a `TypeVar` in the AST is as follows:
`TypeVar(identifier name, expr? bound)`. But as we discussed above, `TypeVar`\ s can
also have constraints, represented syntactically as a tuple of types.

How do we distinguish between them? One option could be to push the distinction all the
way to the runtime: we evaluate the value, and if it is a tuple, we treat it as a set of
constraints, and otherwise we treat it as a bound. This option would lead to a subtle
incompatibility between static and runtime type checking:

```python
types = (bytes, str)

def func[T: types](): ...
```

A static type checker would reject this code, because `types` looks like a bound but is
not a valid type. However, tool that looks at `func` at runtime would have no good way
to distinguish this invalid function from a valid declaration such as
`def func[T: (bytes, str)](): ...`.

Therefore, we want to distinguish between bounds and constraints at compile time,
without evaluating the expression. One way to do that would be to push the distinction
to the grammar and have separate grammar rules for bounds and constraints. I didn't try
to implement that, but I expect it would be relatively complex, as we'd have to
re-implement tuple parsing just for this case.

Instead, we distinguish between the two cases later, in the compiler: if the expression
in the `bound` field is syntactically a tuple (an instance of `ast.Tuple`), we treat it
as a set of constraints, otherwise we treat it as a bound.

## Lazy evaluation

When annotations were introduced in Python 3.0, they were evaluated eagerly in the scope
in which they are defined. This is easy to understand and implement, but it caused
problems when annotations became widely used for typing. Users with large codebases saw
that a big chunk of their import time was spent evaluating type annotations that they'd
never look at again, because they were used only for static typing. In addition, type
annotations often refer to names that haven't yet been defined when the annotation is
evaluated. For example, users expect to be able to use the name of a class as a type
annotation within the class itself, but when the class body executes, the name of the
class is not yet defined. The workaround for this problem was to write the class name in
a string: `"Class"`.

To fix these problems, [PEP 563](https://peps.python.org/pep-0563/) introduced a
mechanism that turns all annotations into strings in the compiler, so that annotations
become much cheaper and users don't have to worry about whether names they use in
annotations are defined at runtime. This was scheduled to become the only behavior in
Python 3.10, but it was belatedly discovered that the change would break some runtime
uses of type annotations, which had become unexpectedly popular. An alternative
solution, [PEP 649](https://peps.python.org/pep-0649/) was introduced. It essentially
wraps all annotations in a function that is evaluated only on demand. This gets us the
best of both worlds: we don't pay the cost to evaluate all the annotations at import
time, we can use forward references without issues, and we can still introspect type
annotations at runtime. This behavior is slated to be implemented in Python 3.13.

PEP 695 introduces two syntactic contexts that are similar to annotations in that they
are expected to contain types: the value of type aliases and the bounds or constraints
of type variables. Initially, the PEP proposed to evaluate these eagerly, with a
special-cased mechanism to support evaluation of recursive type aliases. However, when I
started implementing the PEP, I felt this risked repeating the same mistakes that we
made with type annotations, and those mistakes are expected to take many more years to
fully fix. By this time, it was clear that some variation of PEP 649 was likely to be
the long-term future for type annotations, so it made sense to use PEP 649-like
semantics for these new syntactic constructs from the beginning.

For both type aliases and bounds, there are common use cases where forward references
are required. Most obviously, type aliases may be recursive:

```python
type Expr = int | Add[Expr, Expr] | Subtract[Expr, Expr]
```

The original version of the PEP handled this with a special case, where the name of the
type alias was already defined during the evaluation of the value. However, this trick
would not work if there are multiple mutually recursive type aliases:

```python
from typing import Literal

type BinOp = Literal["+", "-"]
type LeftParen = Literal["("]
type RightParen = Literal[")"]
type SimpleExpr = int | Parenthesized
type Parenthesized = tuple[LeftParen, Expr, RightParen]
type Expr = SimpleExpr | tuple[SimpleExpr, BinOp, Expr]
```

This set of types represents a simple grammar supporting integers literals, binary
operators (`a + b` and `a - b`), and parenthesized expressions (`a - (b - c)`).

Forward references have historically been very common in `TypeVar` bounds to represent
methods that return instances of the current class. The `Self` special form from
[PEP 673](https://peps.python.org/pep-0673/) has made many of these bounds obsolete, but
forward references in bounds remain occasionally useful. For example, consider
interconnected `Connection` and `Cursor` classes implementing a database engine:

```python
class Connection:
    # Allow the user to provide a subclass of Cursor to control cursor creation
    def cursor[CursorT: Cursor](self, cursor_class: type[CursorT]) -> CursorT:
        ...

class Cursor:
    @property
    def connection(self) -> Connection:
        ...
```

Both classes have type annotations that refer to each other, and lazy evaluation of the
bound and the type annotations leaves the programmer free to order the two classes in
whatever way they prefer.

The initial implementation of lazy evaluation is relatively simple: when a type alias
object is created, we create a function that encapsulates the evaluation of the value of
the alias. This function is stored on the type alias object, and called when the user
requests the value of the type alias (e.g., by accessing the `.__value__` attribute).
The same idea applies to `TypeVar` bounds and constraints.

PEP 649 introduces more advanced mechanisms for evaluating annotations that deal with
cases where some of the names may not be defined and with use cases that prefer a
stringified version of the annotations. When PEP 649 is implemented, we will add similar
evaluation mechanisms to PEP 695's lazily evaluated values, so that users can treat them
in the same way as they treat annotations.

## Scoping

### Requirements

The new scoping rules are the most subtle aspect of the runtime implementation of the
PEP. Indeed, the PEP says that "The lexical scope introduced by the new type parameter
syntax is unlike traditional scopes". The scoping semantics are motivated by several
important use cases:

- Multiple objects in the same module must be able to use the same type parameter names
  without interfering with each other. If we define both `def max[T](...): ...` and
  `def min[T](...): ...` in one module, the two type parameters `T` should be
  independent of each other and scoped only to the two function declarations. This helps
  users understand the semantics that type checkers use for type parameters.
- Type parameters must be usable within the scope of the generic. A function that is
  generic over `T` must be able to use `T` not only in its type annotations, but also in
  annotations for its local variables or for nested functions. A class that is generic
  over `T` may use `T` in its list of bases, in the annotations for class variables and
  methods, and for local variables and nested functions within its methods.
- As discussed above, the values of type aliases and the bounds/constraints of type
  variables should be lazily evaluated, but evaluating them later should behave as
  expected.
- Type parameters must be usable at runtime: code within the lexical scope of a type
  parameter must be able to use the parameter's name and get a usable object back. This
  is important for tools that access types at runtime.

I wrote a new section of the PEP,
[Scoping Behavior](https://peps.python.org/pep-0695/#scoping-behavior), that describes
the intended scoping semantics in detail. It was informed heavily by the implementation
strategy I ended up adopting, which is what we'll discuss next.

### Implementation strategies

While implementing the PEP, I went through several possible implementation strategies
before I landed on one that worked well:

- The PEP itself suggests treating type parameters as an "overlay" over the scope in
  which they are defined, which temporarily adds some new names that are only visible
  within the "overlay" scope. I had trouble getting this to work because to be visible
  in inner scopes, the names needed to be cell variables, and the overlay scope didn't
  provide an obvious place to put the cell variables.
- So perhaps instead we should put the type parameters in the enclosing scope, but
  mangle their names so that they are unique. I tried this, but because of the design of
  Python's symbol table implementation, it proved difficult to get the mangling right in
  all nested scopes. There are also edge cases where I'm not even sure this could be
  implemented correctly, such as if a name is conditionally defined in a class
  namespace.

### Lambda lifting

I eventually landed on a technique where we use a special, immediately evaluated
function to define the type parameters. I later learned this technique is called "lambda
lifting". Essentially, we desugar:

```python
def func[T](arg: T): ...
```

Into:

```python
def __generic_parameters_of_func():
    T = TypeVar("T")
    def func(arg: T): ...
    return func
func = __generic_parameters_of_func()
```

The implementation of the PEP uses hidden functions like these in a number of contexts:

- The type parameters of generic functions. Here, the annotations of the function (but
  not the defaults or the decorators) are evaluated inside the hidden function.
- The type parameters of generic classes. The class's bases and keyword arguments to the
  metaclass are evaluated inside the hidden function, but the decorators are not.
- The type parameters of generic type aliases.
- The lazily evaluated values: `TypeVar` bounds and constraints and type alias values.
  Unlike the other hidden functions, these are not immediately called upon creation.

For the most part, this gives us the semantics we need without the introduction of
complex new concepts into the symtable and compiler: the semantics for function scopes
and nonlocals already work in the way that we want scopes for type parameters to work.

However, there was one big wrinkle and a few smaller ones. Let's start with the big one:
class scopes.

### Class scopes

Class scopes are special in Python: names defined in a class scope are not visible in
other scopes nested within them. For example, this fails:

```pycon
>>> class X:
...     T = int
...     def f(self): print(T)
...
>>> X().f()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in f
NameError: name 'T' is not defined. Did you mean: 'self.T'?
```

This creates a problem for our implementation, because we rely so much on hidden
functions. For example, this code would fail:

```python
class Outer:
    class Nested:
        pass

    type Alias = Nested  # NameError when evaluated

    def meth1[T: Nested](self): pass  # NameError when the bound of T is evaluated

    def meth2[T](self, arg: Nested): pass  # NameError when the annotations are evaluated
```

It's a bit of an edge case, but people really do use nested classes and type aliases
defined at class scope, so we have to make the above code work and resolve the name
`Nested` to the nested class. Therefore, we have to make PEP 695 hidden functions work a
little differently from normal functions: if a hidden function is immediately within a
class, it needs to have access to names defined within the class scope. An analogous
problem occurs with lazily evaluated annotations under PEP 649, so Larry Hastings had
already thought about this problem, and the eventual solution owes a lot to his ideas
and Carl Meyer's.

Implementing this change requires modifications in three places:

- In the symbol table (`symtable.c`), where we track the names defined in each scope and
  figure out whether each name is a global, a local, or something else, we mark the
  scopes that need this special treatment with a flag, `ste_type_params_in_class`.
- In the compiler (`compile.c`), we emit special opcodes if we encounter free variables
  in `ste_type_params_in_class` scopes. These opcodes first check the class dictionary
  for a name, and if the name is not present there, they check the globals.
- In the bytecode interpreter (`ceval.c`), we add implementations of the new opcodes.

But wait! We said that we would look in the class dictionary first. How do we get to
that class dictionary? There is no existing way to get to it from an arbitrary function.
Larry's initial idea was to add a new field, `__locals__`, to the function. Then when we
create a function that needs access to this extra dict, we do the equivalent of
`func.__locals__ = locals()`, and inside the implementation of the new opcodes we look
at `func.__locals__` to resolve names. This works and I implemented it, but we realized
there was a problem. What should the following code do?

```python
class Cls:
    T = "before"
    type Alias = T
Cls.T = "after"
print(Cls.Alias.__value__)
```

Recall that the `__value__` of a type alias is lazily evaluated, so when we access
`Cls.Alias.__value__`, we resolve the name `T` for the first time. You might expect,
then, that this will print "after": that's what's in the class namespace at the time the
alias is evaluated.

In fact, this would have printed "before" in this initial implementation, for a subtle
reason: the class namespace that exists while the class body is evaluated is not the
same as the namespace that exists when the class is fully created. The default
metaclass, `type`, creates a copy of the class namespace that it then uses as the
`__dict__` for the newly created class. I believe this was done as a result of the
addition of the `__prepare__` method ([PEP 3115](https://peps.python.org/pep-3115/)),
which allows the class namespace to be some sort of non-`dict` mapping. Whatever the
origins of this behavior, we decided we had to work around it to make class namespaces
behave in a consistent manner.

Happily, Larry and Carl also suggested a solution, and even more happily, this solution
turned out to be simpler than the previous one! Instead of an attribute on the function
object, we use a new cell variable, `__classdict__`. When the class body executes, we
set this cell variable to the current class namespace (in pseudocode:
`__classdict__ = locals()`). Inside the hidden functions, we make name accesses use the
mapping from the `__classdict__` cell. Roughly speaking, we implement loads as
`__classdict__.get(NAME, globals().get(NAME))`. To point the `__classdict__` to the
namespace of the real class after it is created, we have code in the implementation of
`type` that updates the value in the `__classdict__` cell.

If you're familiar with CPython internals, you might have noticed that this is similar
to how the `__class__` cell works, which is what powers zero-argument `super()`. Like
`__classdict__`, this cell is created by the compiler when it encounters certain
function scopes inside a class (in this case, scopes that use the name `super`).
However, `__class__` holds a reference to the class itself, not to the class's dict, and
it is set only when the class is fully evaluated, not while the class namespace is
executing. Therefore, we were not quite able to reuse the `__class__` cell, but a lot of
the implementation is similar between the two cells.

Below, we'll talk more about how the class scope behavior is implemented at runtime in
the interpreter.

### Other issues

There are a few more subtleties to deal with. First, what should the following code do:

```python
type T = (yield 3)

def func[T: (x := 4)](arg: (y := 5)):
    pass

async def async_func():
    def inner[T: await asyncio.sleep(1)]():
        pass
```

Nobody wants to have to think about that, so we disallow `yield`, `yield from`, `await`,
and `:=` (the walrus operator) in all hidden functions.

That did lead to some implementation complexity, however. We want to give good error
messages for all these cases, without using an obscure term like "hidden function":

```pycon
>>> type T = (yield 3)
  File "<stdin>", line 1
SyntaxError: 'yield expression' can not be used within a type alias
>>> def func[T: (x:=3)](): pass
...
  File "<stdin>", line 1
SyntaxError: 'named expression' can not be used within a TypeVar bound
```

To do that, I had to create separate scope types for type alias values, generic
parameters, and TypeVar bounds, so that the error messages can distinguish between these
three cases. Apart from the error messages, all these scopes behave identically.

Second, if `def func[T](): pass` now actually creates `func` within a hidden nested
function, its qualified name would now become something like
`<generic parameters of func>.<locals>.func`. We don't want that, so we adjust the
compiler mechanism to compute the qualname so that it produces simply `func`.

Last, the PEP specifies that the `nonlocal` statement cannot be used to rebind type
parameters. In other words, this is a syntax error:

```python
def func[T]():
    nonlocal T
    T = "not a TypeVar any more"
```

I personally might have preferred to allow this at runtime: while it doesn't make any
sense to rebind a type parameter using `nonlocal`, it also doesn't break any invariants
in the runtime, and in Tim Peters's words, "special cases aren't special enough to break
the rules". Nevertheless, this rule was in the PEP as accepted, so I set out to
implement it. This required changes in `symtable.c`, where we process scoping
information. My initial implementation had a few problems, but fortunately Carl Meyer
suggested a better approach.

## Interpreter

- New opcodes for class scopes
- Many new intrinsics

## Runtime objects

- TypeVar etc. now in C
- Aiming for full compatibility
- Delegate to Python for some operations (most of all, for Generic)

## Acknowledgments

Eric Traut wrote PEP 695 and implemented an initial prototype
